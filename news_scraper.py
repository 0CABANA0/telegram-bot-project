"""
news_scraper.py - ë„¤ì´ë²„ ë‰´ìŠ¤ ìŠ¤í¬ë˜í¼
ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ì—ì„œ í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
2025~ ë„¤ì´ë²„ SDS ë””ìì¸ ì‹œìŠ¤í…œ ëŒ€ì‘ + ë ˆê±°ì‹œ í´ë°± ì§€ì›.
MD5 í•´ì‹œ ê¸°ë°˜ ì¤‘ë³µ í•„í„°ë§ í¬í•¨.
"""

import hashlib
import json
import random
import re
import time
from datetime import datetime
from pathlib import Path

import requests
from bs4 import BeautifulSoup

from config import NEWS_HASH_FILE

HASH_FILE = Path(NEWS_HASH_FILE)

# ì¡°ë¥˜ì¸í”Œë£¨ì—”ì(AI) ê´€ë ¨ ì œì™¸ í‚¤ì›Œë“œ
_AVIAN_FLU_KEYWORDS = [
    "ì¡°ë¥˜ì¸í”Œë£¨ì—”ì", "ì¡°ë¥˜ë…ê°", "ê³ ë³‘ì›ì„±", "AI ë°©ì—­", "AI ë°œìƒ",
    "AI í™•ì‚°", "ì‚´ì²˜ë¶„", "ê°€ê¸ˆë¥˜", "ë‹­Â·ì˜¤ë¦¬", "AI ì˜ì‹¬",
    "AI í™•ì§„", "ì¡°ë¥˜ ì¸í”Œë£¨ì—”ì", "H5N1", "H5N6", "H5N8",
    "AI ì–‘ì„±", "ì² ìƒˆ", "AI ì—­í•™", "êµ¬ì œì—­",
]

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/131.0.0.0 Safari/537.36"
    ),
    "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
    "Referer": "https://search.naver.com/",
}


def _article_hash(title: str, link: str) -> str:
    """ê¸°ì‚¬ ì œëª©+ë§í¬ì˜ MD5 í•´ì‹œ ìƒì„± (ì¤‘ë³µ íŒë³„ìš©)"""
    raw = f"{title.strip()}|{link.strip()}"
    return hashlib.md5(raw.encode("utf-8")).hexdigest()


def _load_sent_hashes() -> set:
    """ì´ì „ì— ë°œì†¡í•œ ê¸°ì‚¬ í•´ì‹œ ëª©ë¡ ë¡œë“œ"""
    if not HASH_FILE.exists():
        return set()
    try:
        data = json.loads(HASH_FILE.read_text(encoding="utf-8"))
        return set(data.get("hashes", []))
    except (json.JSONDecodeError, KeyError):
        return set()


def _save_sent_hashes(hashes: set):
    """ë°œì†¡í•œ ê¸°ì‚¬ í•´ì‹œ ì €ì¥ (ìµœê·¼ 2000ê°œë§Œ ìœ ì§€)"""
    hash_list = list(hashes)[-2000:]
    HASH_FILE.write_text(
        json.dumps({"updated": datetime.now().isoformat(), "hashes": hash_list},
                    ensure_ascii=False),
        encoding="utf-8",
    )


def _is_avian_flu(article: dict) -> bool:
    """ì¡°ë¥˜ì¸í”Œë£¨ì—”ì ê´€ë ¨ ê¸°ì‚¬ì¸ì§€ íŒë³„"""
    text = f"{article.get('title', '')} {article.get('summary', '')}".lower()
    return any(kw in text for kw in _AVIAN_FLU_KEYWORDS)


def scrape_naver_news(keyword: str, count: int = 5) -> list[dict]:
    """
    ë„¤ì´ë²„ ë‰´ìŠ¤ì—ì„œ í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    ì—¬ëŸ¬ ì „ëµ(SDS ì‹ ê·œ ë””ìì¸ / ë ˆê±°ì‹œ)ì„ ì‹œë„í•˜ì—¬ ì•ˆì •ì ìœ¼ë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤.
    ì¸ê³µì§€ëŠ¥/AI í‚¤ì›Œë“œ ê²€ìƒ‰ ì‹œ ì¡°ë¥˜ì¸í”Œë£¨ì—”ì ê¸°ì‚¬ë¥¼ ìë™ ì œì™¸í•©ë‹ˆë‹¤.

    Returns:
        list[dict]: [{"title", "link", "press", "summary"}, ...]
    """
    url = "https://search.naver.com/search.naver"
    params = {"where": "news", "query": keyword, "sort": "1"}  # sort=1: ìµœì‹ ìˆœ

    # ìµœëŒ€ 2íšŒ ì¬ì‹œë„ (403 ì°¨ë‹¨ ëŒ€ì‘)
    soup = None
    resp = None
    for attempt in range(3):
        try:
            resp = requests.get(url, params=params, headers=HEADERS, timeout=15)
            resp.raise_for_status()
            soup = BeautifulSoup(resp.text, "html.parser")
            break
        except requests.RequestException as e:
            if resp is not None and resp.status_code == 403 and attempt < 2:
                wait = random.uniform(3.0, 6.0)
                print(f"    [{keyword}] 403 ì°¨ë‹¨ â†’ {wait:.1f}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„ ({attempt+1}/2)")
                time.sleep(wait)
                continue
            print(f"[ERROR] '{keyword}' ë‰´ìŠ¤ ìš”ì²­ ì‹¤íŒ¨: {e}")
            return []

    if soup is None:
        return []

    # ì—¬ìœ ë¶„ í™•ë³´ (í•„í„°ë§ìœ¼ë¡œ ê°ì†Œí•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ)
    fetch_count = count + 5

    # ì „ëµ 1: SDS ë””ìì¸ ì‹œìŠ¤í…œ (2025~)
    articles = _parse_sds(soup, fetch_count)

    # ì „ëµ 2: ë ˆê±°ì‹œ ë””ìì¸ (div.news_area)
    if not articles:
        articles = _parse_legacy(soup, fetch_count)

    # ì „ëµ 3: ë²”ìš© ë§í¬ ê¸°ë°˜ ì¶”ì¶œ
    if not articles:
        articles = _parse_generic(soup, fetch_count)

    # ì¡°ë¥˜ì¸í”Œë£¨ì—”ì í•„í„°: AI/ì¸ê³µì§€ëŠ¥ ê´€ë ¨ í‚¤ì›Œë“œì¼ ë•Œ ì ìš©
    ai_keywords = {"ai", "ì¸ê³µì§€ëŠ¥", "ìƒì„±ai", "ìƒì„±í˜•ai", "aië°˜ë„ì²´"}
    if keyword.lower().replace(" ", "") in ai_keywords:
        filtered = [a for a in articles if not _is_avian_flu(a)]
        removed = len(articles) - len(filtered)
        if removed > 0:
            print(f"    [í•„í„°] ì¡°ë¥˜ì¸í”Œë£¨ì—”ì ê¸°ì‚¬ {removed}ê±´ ì œì™¸")
        articles = filtered

    return articles[:count]


def _parse_sds(soup: BeautifulSoup, count: int) -> list[dict]:
    """
    SDS ë””ìì¸ ì‹œìŠ¤í…œ íŒŒì„œ (2025~ ë„¤ì´ë²„ ê²€ìƒ‰ UI).
    Profile ìš”ì†Œ([data-sds-comp="Profile"])ë¥¼ ê¸°ì‚¬ ê²½ê³„ë¡œ ì‚¬ìš©í•˜ì—¬
    ê° ê¸°ì‚¬ì˜ ì–¸ë¡ ì‚¬/ì œëª©/ìš”ì•½/ë§í¬ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    results = []

    # ë‰´ìŠ¤ ì¹´ë“œ ì»¨í…Œì´ë„ˆ ì°¾ê¸°
    container = soup.select_one("div.fds-news-item-list-tab")
    if not container:
        container = soup.select_one("ul.list_news")
    if not container:
        return results

    all_links = container.select("a[href]")
    if not all_links:
        return results

    # Profile ìš”ì†Œì˜ ìœ„ì¹˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê¸°ì‚¬ ê²½ê³„ ì„¤ì •
    # ê° Profileì€ ê¸°ì‚¬ ì‹œì‘ì„ ì˜ë¯¸í•¨
    profiles = container.select('[data-sds-comp="Profile"]')
    if not profiles:
        return results

    # Profileì˜ ë¶€ëª¨ì—ì„œ ì–¸ë¡ ì‚¬ ì´ë¦„ì´ í¬í•¨ëœ í…ìŠ¤íŠ¸ ë§í¬ ì°¾ê¸°
    # ê° Profile ìš”ì†Œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê¸°ì‚¬ ë‹¨ìœ„ íŒŒì‹±
    _SKIP = {"keep.naver.com", "#", ""}

    for profile in profiles:
        if len(results) >= count:
            break

        # Profileì˜ ë°”ë¡œ ìœ„ ë¶€ëª¨ê°€ ê¸°ì‚¬ ì»¨í…Œì´ë„ˆ (sds-comps-full-layout)
        article_el = profile.parent
        if not article_el or not article_el.name:
            continue

        links = article_el.select("a[href]")

        press = ""
        title = ""
        link = ""
        summary = ""

        for a in links:
            href = a.get("href", "").strip()
            text = a.get_text(strip=True)

            # Keep / ë¹ˆ href ìŠ¤í‚µ
            if href in _SKIP or "keep.naver.com" in href:
                continue

            # ë¹ˆ í…ìŠ¤íŠ¸ ìŠ¤í‚µ
            if not text:
                continue

            # "ë„¤ì´ë²„ë‰´ìŠ¤" ë³´ì¡° ë§í¬ ìŠ¤í‚µ
            if text == "ë„¤ì´ë²„ë‰´ìŠ¤":
                continue

            # ì–¸ë¡ ì‚¬ëª…: media.naver.com/press ë§í¬ ë˜ëŠ” ì–¸ë¡ ì‚¬ ìì²´ ë„ë©”ì¸ ë§í¬
            # (ì œëª©ë³´ë‹¤ ë¨¼ì € ë‚˜ì˜¤ëŠ” ì§§ì€ í…ìŠ¤íŠ¸ ë§í¬)
            if not press and not title:
                # ì–¸ë¡ ì‚¬ëª…ì€ ë³´í†µ ì§§ê³  (15ì ì´í•˜), ì œëª©/ìš”ì•½ë³´ë‹¤ ë¨¼ì € ë‚˜ì˜´
                if "media.naver.com/press" in href or len(text) <= 15:
                    press = text
                    continue

            # ì œëª©: ì²« ë²ˆì§¸ ì½˜í…ì¸  ë§í¬ (5ì ì´ìƒ)
            if not title and len(text) >= 5:
                title = text
                link = href
                continue

            # ìš”ì•½: ë‘ ë²ˆì§¸ ì½˜í…ì¸  ë§í¬ (20ì ì´ìƒ)
            if title and not summary and len(text) >= 10:
                summary = text
                continue

        if title and link:
            results.append({
                "title": title,
                "link": link,
                "press": press,
                "summary": summary[:120] if summary else "",
            })

    return results


def _parse_legacy(soup: BeautifulSoup, count: int) -> list[dict]:
    """ë ˆê±°ì‹œ ë””ìì¸ íŒŒì„œ (div.news_area ê¸°ë°˜)"""
    results = []
    articles = soup.select("div.news_area")[:count]

    for article in articles:
        title_tag = article.select_one("a.news_tit")
        if not title_tag:
            continue

        title = title_tag.get_text(strip=True)
        link = title_tag.get("href", "")

        press_tag = article.select_one("a.info.press")
        press = press_tag.get_text(strip=True) if press_tag else ""

        summary_tag = article.select_one("div.news_dsc")
        summary = summary_tag.get_text(strip=True)[:120] if summary_tag else ""

        results.append({
            "title": title,
            "link": link,
            "press": press,
            "summary": summary,
        })

    return results


def _parse_generic(soup: BeautifulSoup, count: int) -> list[dict]:
    """ë²”ìš© í´ë°± íŒŒì„œ: news.naver.com ë§í¬ë¥¼ ì§ì ‘ íƒìƒ‰"""
    results = []
    seen_links = set()

    for a_tag in soup.select('a[href*="news.naver.com"]'):
        href = a_tag.get("href", "")
        text = a_tag.get_text(strip=True)

        if not text or len(text) < 5 or href in seen_links:
            continue

        # ê¸°ì‚¬ ë§í¬ íŒ¨í„´ë§Œ í—ˆìš©
        if "/article/" not in href and "/read" not in href:
            continue

        seen_links.add(href)
        results.append({
            "title": text,
            "link": href,
            "press": "",
            "summary": "",
        })

        if len(results) >= count:
            break

    return results


def scrape_all_keywords(keywords: list[str], count_per: int = 5) -> dict[str, list[dict]]:
    """
    ì—¬ëŸ¬ í‚¤ì›Œë“œë¥¼ í•œ ë²ˆì— ìŠ¤í¬ë˜í•‘í•©ë‹ˆë‹¤.
    ì¤‘ë³µ ê¸°ì‚¬ë¥¼ MD5 í•´ì‹œë¡œ í•„í„°ë§í•©ë‹ˆë‹¤.
    ë„¤ì´ë²„ ì°¨ë‹¨ ë°©ì§€ë¥¼ ìœ„í•´ ìš”ì²­ ê°„ ëœë¤ ë”œë ˆì´ë¥¼ ì ìš©í•©ë‹ˆë‹¤.

    Returns:
        dict: {í‚¤ì›Œë“œ: [ê¸°ì‚¬ ëª©ë¡], ...}
    """
    sent_hashes = _load_sent_hashes()
    all_results = {}
    new_hashes = set()

    for idx, keyword in enumerate(keywords):
        # ë„¤ì´ë²„ 403 ì°¨ë‹¨ ë°©ì§€: ìš”ì²­ ê°„ 1~3ì´ˆ ëœë¤ ë”œë ˆì´
        if idx > 0:
            delay = random.uniform(1.0, 3.0)
            time.sleep(delay)
        raw_articles = scrape_naver_news(keyword, count=count_per + 3)  # ì—¬ìœ ë¶„
        filtered = []

        for article in raw_articles:
            h = _article_hash(article["title"], article["link"])
            if h not in sent_hashes and h not in new_hashes:
                filtered.append(article)
                new_hashes.add(h)

            if len(filtered) >= count_per:
                break

        all_results[keyword] = filtered
        total = len(filtered)
        print(f"  [{keyword}] {total}ê±´ ìˆ˜ì§‘ (ì›ë³¸ {len(raw_articles)}ê±´)")

    # í•´ì‹œ ì €ì¥
    sent_hashes.update(new_hashes)
    _save_sent_hashes(sent_hashes)

    return all_results


def format_news_for_telegram(all_news: dict[str, list[dict]]) -> str:
    """
    í‚¤ì›Œë“œë³„ ë‰´ìŠ¤ë¥¼ í…”ë ˆê·¸ë¨ HTML ë©”ì‹œì§€ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

    Args:
        all_news: scrape_all_keywords()ì˜ ë°˜í™˜ê°’

    Returns:
        str: HTML í˜•ì‹ì˜ í…”ë ˆê·¸ë¨ ë©”ì‹œì§€
    """
    now = datetime.now()
    time_label = "ì˜¤ì „" if now.hour < 12 else "ì˜¤í›„"
    date_str = now.strftime("%Y-%m-%d")

    lines = [
        f"<b>ğŸ“° ë„¤ì´ë²„ ë‰´ìŠ¤ ë¸Œë¦¬í•‘</b>  ({date_str} {time_label})",
        "",
    ]

    total_count = 0

    for keyword, articles in all_news.items():
        if not articles:
            lines.append(f"ğŸ”¹ <b>{keyword}</b> â€” ìƒˆë¡œìš´ ë‰´ìŠ¤ ì—†ìŒ")
            lines.append("")
            continue

        lines.append(f"ğŸ”¹ <b>{keyword}</b>")

        for i, art in enumerate(articles, 1):
            press_str = f" [{art['press']}]" if art.get("press") else ""
            title_escaped = _escape_html(art["title"])
            lines.append(
                f'  {i}. <a href="{art["link"]}">{title_escaped}</a>{press_str}'
            )
            if art.get("summary"):
                summary_escaped = _escape_html(art["summary"][:80])
                lines.append(f"     {summary_escaped}")

        lines.append("")
        total_count += len(articles)

    lines.append(f"ğŸ“Š ì´ {total_count}ê±´")

    return "\n".join(lines)


def _escape_html(text: str) -> str:
    """í…”ë ˆê·¸ë¨ HTMLì—ì„œ íŠ¹ìˆ˜ë¬¸ì ì´ìŠ¤ì¼€ì´í”„"""
    return (
        text.replace("&", "&amp;")
        .replace("<", "&lt;")
        .replace(">", "&gt;")
    )
